{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d65140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Handwritten_Digit_Recogniser\\dig_rec_env\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHhxJREFUeJzt3QtwVOX5x/EnIAnXBEOAJBDuNwUCLUKkIBdBIFVGInZAcQotAwWDw6WAjS03WxsFQQdFYKaWSFVQWgGhTiwQktRysYDIUJQSBkuQAIpNAsEETM5/3vc/SbOQgGdJ8mx2v5+ZM5vdPc/uycnJ+e17zrvvCXIcxxEAAGpYnZp+QwAACCAAgBpaQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAG36YsvvpCgoCB58cUXq2xdpqen29c0t4C/IoAQkFJSUuwO/sCBA+KPFi9ebH+/66f69etrLxpQ5o7//QjA36xevVoaN25cdr9u3bqqywOURwABfuzRRx+ViIgI7cUAKsQhOKASV69elYULF0qfPn0kLCxMGjVqJPfdd5/s3r270nX20ksvSdu2baVBgwYyePBgOXr06A3zfP755zYYwsPD7SGxe+65R95///1b/h2uXLlia7/++uvv/Tczg93n5+fbW8DXEEBAJcyO+w9/+IMMGTJEXnjhBXte5auvvpKRI0fK4cOHb5h//fr1snLlSklMTJSkpCQbPvfff7+cP3++bJ5//etfcu+998pnn30mv/rVr2T58uU22MaMGSObN2++6d/i448/lrvuukteffXV7/0369Chgw3PJk2ayBNPPOGxLIA2DsEBlbjzzjttD7fg4OCyx6ZMmSLdunWTV155RV5//XWP+bOysuTEiRPSqlUre3/UqFESFxdnw2vFihX2sZkzZ0qbNm3kn//8p4SEhNjHnnzySRk4cKA8/fTTkpCQUGXLPmPGDOnfv799n7///e+yatUqG2Km40VoaCh/d6gjgIBKmBP2pSftS0pKJDc3196aQ2aHDh26YX7TiikNH6Nfv342gD744AMbQN98842kpaXJs88+K5cuXbJTKdOqWrRokXz55Zcer1GeaYl930NpJujKGzt2rF2eCRMmyGuvvWZbX4A2DsEBN/HGG29IbGysPVfTrFkzad68ufz1r3+VvLy8G+bt3LnzDY916dLFtqJKW0gmQBYsWGBfp/xkwse4cOFCtf09Hn/8cYmMjJSdO3dW23sAbtACAirx5ptvyqRJk2zLZt68edKiRQvbIkpOTpaTJ0+6Xm+m9WTMnTvXtngq0qlTp2r9e8TExNiWGOALCCCgEn/+85/tSfz33nvPfomzVGlr5Xrm/M/1/v3vf0u7du3sz+a1jHr16snw4cNrfL2b1pdpjf3gBz+o8fcGKsIhOKASped/yp932b9/v+zdu7fC+bds2WLP4ZQyJ/zN/PHx8fa+aUGZ8zhr166VnJycG+pND7uq6oZd0WuZL6Wax03nCMAX0AJCQPvjH/8oqampFZ7Ef+ihh2zrx/RMe/DBB+XUqVOyZs0aufvuu+Xy5csVHj4zvdmmT58uRUVF8vLLL9vzRvPnzy+bx/REM/P07NnT9qgzrSLTNdqE2pkzZ+TTTz+tdFlNoA0dOtS2wEyX8Jsx30UaN26cfR9z/uqjjz6SjRs3Su/eveUXv/iF6/UEVAcCCAHNtAoqYs79mOncuXO2xfLhhx/a4DHnhTZt2lThIKE//elPpU6dOjZ4TGcC0+vMfGcnKiqqbB7zGqYb9JIlS+x4dBcvXrQtI3NYzHzptaqY3m579uyRv/zlL1JYWGgDyQThr3/9a2nYsGGVvQ9wO4IcviINAFDAOSAAgAoCCACgggACAKgggAAAKgggAIAKAggAoMLnvgdkxss6e/asvX5J+eFPAAC1g/l2jxntPTo62n43rtYEkAkfM2AiAKB2y87OltatW9eeQ3Cm5QMAqP1utT+vtgAyY16ZUYDNOFTmolxmHKvvg8NuAOAfbrU/r5YAeuedd2TOnDl20ERz5chevXrZ659U58W2AAC1jFMN+vXr5yQmJpbdLy4udqKjo53k5ORb1ubl5Zmx75lYB2wDbANsA1K714HZn99MlbeArl69KgcPHvS44JbpBWHuV3QdFTNsfX5+vscEAPB/VR5A5mJZxcXF0rJlS4/HzX0ztP31zOWNw8LCyiZ6wAFAYFDvBZeUlCR5eXllk+m2BwDwf1X+PaCIiAh7KWNzlcfyzP3IyMgb5g8JCbETACCwVHkLKDg4WPr06SO7du3yGN3A3O/fv39Vvx0AoJaqlpEQTBfsiRMnyj333GMvS2wuUVxQUCA/+9nPquPtAAC1ULUE0Lhx4+Srr76y17g3HQ969+4tqampN3RMAAAEriDTF1t8iOmGbXrDAQBqN9OxLDQ01Hd7wQEAAhMBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBAAggAAAgYMWEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABAxR06bwv4prp167quCQsLE181Y8YMr+oaNmzouqZr166uaxITE13XvPjii65rHnvsMfFGYWGh65rnn3/edc2SJUskENECAgCoIIAAAP4RQIsXL5agoCCPqVu3blX9NgCAWq5azgF1795ddu7c+b83uYNTTQAAT9WSDCZwIiMjq+OlAQB+olrOAZ04cUKio6OlQ4cOMmHCBDl9+nSl8xYVFUl+fr7HBADwf1UeQHFxcZKSkiKpqamyevVqOXXqlNx3331y6dKlCudPTk623VhLp5iYmKpeJABAIARQfHy8/OQnP5HY2FgZOXKkfPDBB5KbmyvvvvtuhfMnJSVJXl5e2ZSdnV3ViwQA8EHV3jugadOm0qVLF8nKyqrw+ZCQEDsBAAJLtX8P6PLly3Ly5EmJioqq7rcCAARyAM2dO1cyMjLkiy++kD179khCQoId3sTboTAAAP6pyg/BnTlzxobNxYsXpXnz5jJw4EDZt2+f/RkAgGoLoI0bN1b1S8JHtWnTxnVNcHCw65of/ehHrmvMBx9vz1m6NXbsWK/ey9+YD59urVy50nWNOariVmW9cG/l008/dV1jjgDh+2EsOACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACABBAAIDAQQsIAKCCAAIAqCCAAAAqCCAAgIogx3Ec8SH5+fn20tyoOb179/aqLi0tzXUNf9vaoaSkxHXNz3/+c6+uF1YTcnJyvKr773//67rm+PHjXr2XPzJXuQ4NDa30eVpAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVd+i8LXzJ6dOnvaq7ePGi6xpGw/5/+/fvd73ucnNzXdcMHTrUdY1x9epV1zV/+tOfvHovBC5aQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQwGCnkm2++8WotzJs3z3XNQw895Lrmk08+cV2zcuVKqSmHDx92XfPAAw+4rikoKHBd0717d/HGzJkzvaoD3KAFBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQEWQ4ziO+JD8/HwJCwvTXgxUk9DQUNc1ly5dcl2zdu1a8cbkyZNd1zzxxBOuazZs2OC6Bqht8vLybvo/TwsIAKCCAAIA1I4AyszMlNGjR0t0dLQEBQXJli1bPJ43R/QWLlwoUVFR0qBBAxk+fLicOHGiKpcZABCIAWQuitWrVy9ZtWpVhc8vXbrUXgxszZo1sn//fmnUqJGMHDlSCgsLq2J5AQCBekXU+Ph4O1XEtH5efvll+c1vfiMPP/ywfWz9+vXSsmVL21IaP3787S8xAMAvVOk5oFOnTsm5c+fsYbdSpkdbXFyc7N27t8KaoqIi2/Ot/AQA8H9VGkAmfAzT4inP3C997nrJyck2pEqnmJiYqlwkAICPUu8Fl5SUZPuKl07Z2dnaiwQAqG0BFBkZaW/Pnz/v8bi5X/rc9UJCQuwXlcpPAAD/V6UB1L59exs0u3btKnvMnNMxveH69+9flW8FAAi0XnCXL1+WrKwsj44Hhw8flvDwcGnTpo3MmjVLfve730nnzp1tIC1YsMB+Z2jMmDFVvewAgEAKoAMHDsjQoUPL7s+ZM8feTpw4UVJSUmT+/Pn2u0JTp06V3NxcGThwoKSmpkr9+vWrdskBALUag5HCLy1btsyrutIPVG5kZGS4rin/VYXvq6SkxHUNoInBSAEAPkm9GzYAIDARQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQwWjY8EuNGjXyqm7btm2uawYPHuy6Jj4+3nXN3/72N9c1gCZGwwYA+CQOwQEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABYORAuV07NjR9fo4dOiQ65rc3FzXNbt373Zdc+DAAfHGqlWrXNc4juPVe8F/MRgpAMAncQgOAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoYjBS4TQkJCa5r1q1b57qmSZMmUlOeeeYZ1zXr1693XZOTk+O6BrUHg5ECAHwSh+AAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoILBSAEFPXr0cF2zYsUK1zXDhg2TmrJ27VrXNc8995zrmi+//NJ1DXQwGCkAwCdxCA4AUDsCKDMzU0aPHi3R0dESFBQkW7Zs8Xh+0qRJ9vHy06hRo6pymQEAgRhABQUF0qtXL1m1alWl85jAMReaKp02bNhwu8sJAPAzd7gtiI+Pt9PNhISESGRk5O0sFwDAz1XLOaD09HRp0aKFdO3aVaZPny4XL16sdN6ioiLJz8/3mAAA/q/KA8gcfjPXht+1a5e88MILkpGRYVtMxcXFFc6fnJwsYWFhZVNMTExVLxIAwB8Owd3K+PHjy37u2bOnxMbGSseOHW2rqKLvJCQlJcmcOXPK7psWECEEAP6v2rthd+jQQSIiIiQrK6vS80WhoaEeEwDA/1V7AJ05c8aeA4qKiqrutwIA+PMhuMuXL3u0Zk6dOiWHDx+W8PBwOy1ZskTGjh1re8GdPHlS5s+fL506dZKRI0dW9bIDAAIpgA4cOCBDhw4tu196/mbixImyevVqOXLkiLzxxhuSm5trv6w6YsQI+e1vf2sPtQEAUIrBSIFaomnTpq5rzKgl3li3bp3rGjPqiVtpaWmuax544AHXNdDBYKQAAJ/EYKQAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUMBo2gBsUFRW5Xit33OH66i7y3Xffua7x5tpi6enprmtw+xgNGwDgkzgEBwBQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAV7kcPBHDbYmNjXdc8+uijrmv69u0r3vBmYFFvHDt2zHVNZmZmtSwLah4tIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoYjBQop2vXrq7Xx4wZM1zXPPLII65rIiMjxZcVFxe7rsnJyXFdU1JS4roGvokWEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUMRgqf580gnI899phX7+XNwKLt2rUTf3PgwAHXNc8995zrmvfff991DfwHLSAAgAoCCADg+wGUnJwsffv2lSZNmkiLFi1kzJgxcvz4cY95CgsLJTExUZo1ayaNGzeWsWPHyvnz56t6uQEAgRRAGRkZNlz27dsnO3bskGvXrsmIESOkoKCgbJ7Zs2fLtm3bZNOmTXb+s2fPenXxLQCAf3PVCSE1NdXjfkpKim0JHTx4UAYNGiR5eXny+uuvy9tvvy3333+/nWfdunVy11132dC69957q3bpAQCBeQ7IBI4RHh5ub00QmVbR8OHDy+bp1q2btGnTRvbu3VvhaxQVFUl+fr7HBADwf14HkLku+6xZs2TAgAHSo0cP+9i5c+ckODhYmjZt6jFvy5Yt7XOVnVcKCwsrm2JiYrxdJABAIASQORd09OhR2bhx420tQFJSkm1JlU7Z2dm39XoAAD/+Iqr5st727dslMzNTWrdu7fGFwatXr0pubq5HK8j0gqvsy4QhISF2AgAEFlctIMdxbPhs3rxZ0tLSpH379h7P9+nTR+rVqye7du0qe8x00z59+rT079+/6pYaABBYLSBz2M30cNu6dav9LlDpeR1z7qZBgwb2dvLkyTJnzhzbMSE0NFSeeuopGz70gAMAeB1Aq1evtrdDhgzxeNx0tZ40aZL9+aWXXpI6derYL6CaHm4jR46U1157zc3bAAACQJBjjqv5ENMN27Sk4PtM70a37r77btc1r776qusa0/3f3+zfv991zbJly7x6L3OUw5uesUB5pmOZORJWGcaCAwCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgDUniuiwneZ6zC5tXbtWq/eq3fv3q5rOnToIP5mz549rmuWL1/uuubDDz90XfPtt9+6rgFqCi0gAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKhiMtIbExcW5rpk3b57rmn79+rmuadWqlfibK1eueFW3cuVK1zW///3vXdcUFBS4rgH8DS0gAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKhiMtIYkJCTUSE1NOnbsmOua7du3u6757rvvXNcsX75cvJGbm+tVHQD3aAEBAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQEeQ4jiM+JD8/X8LCwrQXAwBwm/Ly8iQ0NLTS52kBAQBUEEAAAN8PoOTkZOnbt680adJEWrRoIWPGjJHjx497zDNkyBAJCgrymKZNm1bVyw0ACKQAysjIkMTERNm3b5/s2LFDrl27JiNGjJCCggKP+aZMmSI5OTll09KlS6t6uQEAgXRF1NTUVI/7KSkptiV08OBBGTRoUNnjDRs2lMjIyKpbSgCA36lzuz0cjPDwcI/H33rrLYmIiJAePXpIUlKSXLlypdLXKCoqsj3fyk8AgADgeKm4uNh58MEHnQEDBng8vnbtWic1NdU5cuSI8+abbzqtWrVyEhISKn2dRYsWmW7gTKwDtgG2AbYB8a91kJeXd9Mc8TqApk2b5rRt29bJzs6+6Xy7du2yC5KVlVXh84WFhXYhSyfzetorjYl1wDbANsA2INUeQK7OAZWaMWOGbN++XTIzM6V169Y3nTcuLs7eZmVlSceOHW94PiQkxE4AgMDiKoBMi+mpp56SzZs3S3p6urRv3/6WNYcPH7a3UVFR3i8lACCwA8h0wX777bdl69at9rtA586ds4+boXMaNGggJ0+etM//+Mc/lmbNmsmRI0dk9uzZtodcbGxsdf0OAIDayM15n8qO861bt84+f/r0aWfQoEFOeHi4ExIS4nTq1MmZN2/eLY8Dlmfm5dgrx9/ZBtgG2AZq/zZwq30/g5ECAKoFg5ECAHwSg5ECAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQ4XMB5DiO9iIAAGpgf+5zAXTp0iXtRQAA1MD+PMjxsSZHSUmJnD17Vpo0aSJBQUEez+Xn50tMTIxkZ2dLaGioBCrWA+uB7YH/C1/eP5hYMeETHR0tdepU3s65Q3yMWdjWrVvfdB6zUgM5gEqxHlgPbA/8X/jq/iEsLOyW8/jcITgAQGAggAAAKmpVAIWEhMiiRYvsbSBjPbAe2B74v/CH/YPPdUIAAASGWtUCAgD4DwIIAKCCAAIAqCCAAAAqCCAAgIpaE0CrVq2Sdu3aSf369SUuLk4+/vhj7UWqcYsXL7bDE5WfunXrJv4uMzNTRo8ebYf1ML/zli1bPJ43HTkXLlwoUVFR0qBBAxk+fLicOHFCAm09TJo06YbtY9SoUeJPkpOTpW/fvnaorhYtWsiYMWPk+PHjHvMUFhZKYmKiNGvWTBo3bixjx46V8+fPS6CthyFDhtywPUybNk18Sa0IoHfeeUfmzJlj+7YfOnRIevXqJSNHjpQLFy5IoOnevbvk5OSUTR999JH4u4KCAvs3Nx9CKrJ06VJZuXKlrFmzRvbv3y+NGjWy24fZEQXSejBM4JTfPjZs2CD+JCMjw4bLvn37ZMeOHXLt2jUZMWKEXTelZs+eLdu2bZNNmzbZ+c3Yko888ogE2nowpkyZ4rE9mP8Vn+LUAv369XMSExPL7hcXFzvR0dFOcnKyE0gWLVrk9OrVywlkZpPdvHlz2f2SkhInMjLSWbZsWdljubm5TkhIiLNhwwYnUNaDMXHiROfhhx92AsmFCxfsusjIyCj729erV8/ZtGlT2TyfffaZnWfv3r1OoKwHY/Dgwc7MmTMdX+bzLaCrV6/KwYMH7WGV8gOWmvt79+6VQGMOLZlDMB06dJAJEybI6dOnJZCdOnVKzp0757F9mEEQzWHaQNw+0tPT7SGZrl27yvTp0+XixYviz/Ly8uxteHi4vTX7CtMaKL89mMPUbdq08evtIe+69VDqrbfekoiICOnRo4ckJSXJlStXxJf43GjY1/v666+luLhYWrZs6fG4uf/5559LIDE71ZSUFLtzMc3pJUuWyH333SdHjx61x4IDkQkfo6Lto/S5QGEOv5lDTe3bt5eTJ0/KM888I/Hx8XbHW7duXfE35tIts2bNkgEDBtgdrGH+5sHBwdK0adOA2R5KKlgPxuOPPy5t27a1H1iPHDkiTz/9tD1P9N5774mv8PkAwv+YnUmp2NhYG0hmA3v33Xdl8uTJrKoAN378+LKfe/bsabeRjh072lbRsGHDxN+YcyDmw1cgnAf1Zj1MnTrVY3swnXTMdmA+nJjtwhf4/CE403w0n96u78Vi7kdGRkogM5/yunTpIllZWRKoSrcBto8bmcO05v/HH7ePGTNmyPbt22X37t0e1w8z24M5bJ+bmxsQ+4sZlayHipgPrIYvbQ8+H0CmOd2nTx/ZtWuXR5PT3O/fv78EssuXL9tPM+aTTaAyh5vMjqX89mGuCGl6wwX69nHmzBl7Dsiftg/T/8LsdDdv3ixpaWn271+e2VfUq1fPY3swh53MuVJ/2h6cW6yHihw+fNje+tT24NQCGzdutL2aUlJSnGPHjjlTp051mjZt6pw7d84JJL/85S+d9PR059SpU84//vEPZ/jw4U5ERITtAePPLl265HzyySd2MpvsihUr7M//+c9/7PPPP/+83R62bt3qHDlyxPYEa9++vfPtt986gbIezHNz5861Pb3M9rFz507nhz/8odO5c2ensLDQ8RfTp093wsLC7P9BTk5O2XTlypWyeaZNm+a0adPGSUtLcw4cOOD079/fTv5k+i3WQ1ZWlvPss8/a399sD+Z/o0OHDs6gQYMcX1IrAsh45ZVX7EYVHBxsu2Xv27fPCTTjxo1zoqKi7Dpo1aqVvW82NH+3e/duu8O9fjLdjku7Yi9YsMBp2bKl/aAybNgw5/jx404grQez4xkxYoTTvHlz2w25bdu2zpQpU/zuQ1pFv7+Z1q1bVzaP+eDx5JNPOnfeeafTsGFDJyEhwe6cA2k9nD592oZNeHi4/Z/o1KmTM2/ePCcvL8/xJVwPCACgwufPAQEA/BMBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEACAAAIABA5aQAAA0fB/j66CPz0vyJYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the built-in MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Show the first image just to see what we're working with\n",
    "plt.imshow(x_train[0], cmap='gray')\n",
    "plt.title(f\"Label: {y_train[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa41859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Reshape for the CNN (Adding the '1' channel for grayscale)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f6f66d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Handwritten_Digit_Recogniser\\dig_rec_env\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    # Step 1: Feature Extraction\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Step 2: Classification\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax') # 10 neurons for digits 0-9\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f79112e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.9580 - loss: 0.1367 - val_accuracy: 0.9813 - val_loss: 0.0562\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9861 - loss: 0.0450 - val_accuracy: 0.9874 - val_loss: 0.0358\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9907 - loss: 0.0311 - val_accuracy: 0.9870 - val_loss: 0.0387\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9929 - loss: 0.0226 - val_accuracy: 0.9897 - val_loss: 0.0318\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9944 - loss: 0.0170 - val_accuracy: 0.9915 - val_loss: 0.0289\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9915 - loss: 0.0289\n",
      "Test Accuracy: 99.15%\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# Check final accuracy\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4d7dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('digit_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fcfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 1. Load the trained model\n",
    "try:\n",
    "    model = load_model('digit_model.h5')\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}. Ensure 'digit_model.h5' is in the same directory.\")\n",
    "\n",
    "def process_and_predict(file_path):\n",
    "    # Load image\n",
    "    img = cv2.imread(file_path)\n",
    "    if img is None: return None, \"\"\n",
    "    \n",
    "    # --- 1. HIGH-RES OPTIMIZATION ---\n",
    "    # Downscale high-res images to a standard width (1000px) \n",
    "    # This makes processing faster and our filters consistent.\n",
    "    h_orig, w_orig = img.shape[:2]\n",
    "    standard_w = 1000\n",
    "    scale = standard_w / w_orig\n",
    "    img = cv2.resize(img, (standard_w, int(h_orig * scale)), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # --- 2. PREPROCESSING ---\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Stronger blur for high-res grain removal\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Use Otsu's to find the best threshold automatically\n",
    "    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Clean background noise and thicken the strokes\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    thresh = cv2.dilate(thresh, kernel, iterations=1)\n",
    "\n",
    "    # --- 3. CONTOUR DETECTION ---\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Sort from left to right\n",
    "    contours = sorted(contours, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "\n",
    "    output_img = img.copy()\n",
    "    detected_digits = []\n",
    "\n",
    "    for ctr in contours:\n",
    "        x, y, w, h = cv2.boundingRect(ctr)\n",
    "        \n",
    "        # Filter based on standard_w = 1000 scale\n",
    "        # If height is at least 4% of the image height, it's likely a digit\n",
    "        if h > (img.shape[0] * 0.04) and w > 10:\n",
    "            roi = thresh[y:y+h, x:x+w]\n",
    "            \n",
    "            # --- 4. ASPECT RATIO PRESERVATION (The square padding) ---\n",
    "            # Instead of resizing ROI directly (which stretches digits), \n",
    "            # we place it on a square black canvas.\n",
    "            pad = 20\n",
    "            dim = max(w, h) + (pad * 2)\n",
    "            square_canvas = np.zeros((dim, dim), dtype=\"uint8\")\n",
    "            \n",
    "            # Center the ROI\n",
    "            start_x = (dim - w) // 2\n",
    "            start_y = (dim - h) // 2\n",
    "            square_canvas[start_y:start_y+h, start_x:start_x+w] = roi\n",
    "            \n",
    "            # --- 5. MODEL PREDICTION ---\n",
    "            final_input = cv2.resize(square_canvas, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "            final_input = final_input.astype(\"float32\") / 255.0\n",
    "            final_input = np.expand_dims(final_input, axis=(0, -1))\n",
    "            \n",
    "            preds = model.predict(final_input, verbose=0)\n",
    "            digit = np.argmax(preds)\n",
    "            confidence = np.max(preds)\n",
    "            \n",
    "            # Confidence Threshold (85% sure)\n",
    "            if confidence > 0.85:\n",
    "                detected_digits.append(str(digit))\n",
    "                cv2.rectangle(output_img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(output_img, f\"{digit}\", (x, y-10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "    return output_img, \"\".join(detected_digits)\n",
    "\n",
    "# --- 6. GUI INTERFACE ---\n",
    "class OCRApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Intelligent Digit Scanner\")\n",
    "        self.root.geometry(\"800x900\")\n",
    "        self.root.configure(bg=\"#1e1e1e\")\n",
    "\n",
    "        tk.Label(root, text=\"AI Digit Recognition System\", fg=\"white\", bg=\"#1e1e1e\", \n",
    "                 font=(\"Segoe UI\", 20, \"bold\")).pack(pady=20)\n",
    "\n",
    "        self.btn = tk.Button(root, text=\"SELECT IMAGE\", command=self.load_image, \n",
    "                             bg=\"#0078d7\", fg=\"white\", font=(\"Segoe UI\", 12, \"bold\"), \n",
    "                             padx=30, pady=12, relief=\"flat\")\n",
    "        self.btn.pack()\n",
    "\n",
    "        self.display = tk.Label(root, bg=\"#2d2d2d\")\n",
    "        self.display.pack(pady=30)\n",
    "\n",
    "        self.status = tk.Label(root, text=\"Waiting for input...\", fg=\"#aaaaaa\", \n",
    "                               bg=\"#1e1e1e\", font=(\"Segoe UI\", 14))\n",
    "        self.status.pack(pady=10)\n",
    "\n",
    "    def load_image(self):\n",
    "        path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png\")])\n",
    "        if not path: return\n",
    "\n",
    "        self.status.config(text=\"Scanning...\", fg=\"#ffcc00\")\n",
    "        self.root.update_idletasks()\n",
    "\n",
    "        res_img, digit_str = process_and_predict(path)\n",
    "\n",
    "        if res_img is not None:\n",
    "            # Convert for Tkinter\n",
    "            res_img = cv2.cvtColor(res_img, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(res_img)\n",
    "            pil_img.thumbnail((600, 600))\n",
    "            tk_img = ImageTk.PhotoImage(pil_img)\n",
    "            \n",
    "            self.display.config(image=tk_img)\n",
    "            self.display.image = tk_img\n",
    "            \n",
    "            if digit_str:\n",
    "                self.status.config(text=f\"DETECTED: {digit_str}\", fg=\"#00ff00\")\n",
    "            else:\n",
    "                self.status.config(text=\"No clear digits found.\", fg=\"#ff4444\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_win = tk.Tk()\n",
    "    app = OCRApp(root_win)\n",
    "    root_win.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dig_rec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
